{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9f34da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.models import Sequential\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "server = 'cyse492group8.database.windows.net'\n",
    "database = 'CYSE492_Project'\n",
    "username = 'cyseadmin'\n",
    "password = 'GroupProject492'\n",
    "conn = pyodbc.connect('DRIVER={SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f875b99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT [Control Identifier],[Control Text]  FROM [dbo].['NIST_800.53_controls$']\")\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "for row in cursor:\n",
    "    texts.append(row[1])\n",
    "    if 'AC' in row[0]:\n",
    "        labels.append(1)\n",
    "    if 'AT' in row[0]:\n",
    "        labels.append(2)\n",
    "    if 'AU' in row[0]:\n",
    "        labels.append(3)\n",
    "    if 'CA' in row[0]:\n",
    "        labels.append(4)\n",
    "    if 'CM' in row[0]:\n",
    "        labels.append(5)\n",
    "    if 'CP' in row[0]:\n",
    "        labels.append(6)\n",
    "    if 'IA' in row[0]:\n",
    "        labels.append(7)\n",
    "    if 'IR' in row[0]:\n",
    "        labels.append(8)\n",
    "    if 'MA' in row[0]:\n",
    "        labels.append(9)\n",
    "    if 'MP' in row[0]:\n",
    "        labels.append(10)\n",
    "    if 'PE' in row[0]:\n",
    "        labels.append(11)\n",
    "    if 'PL' in row[0]:\n",
    "        labels.append(12)\n",
    "    if 'PM' in row[0]:\n",
    "        labels.append(13)\n",
    "    if 'PS' in row[0]:\n",
    "        labels.append(14)\n",
    "    if 'PT' in row[0]:\n",
    "        labels.append(15)\n",
    "    if 'RA' in row[0]:\n",
    "        labels.append(16)\n",
    "    if 'SA' in row[0]:\n",
    "        labels.append(17)\n",
    "    if 'SC' in row[0]:\n",
    "        labels.append(18)\n",
    "    if 'SI' in row[0]:\n",
    "        labels.append(19)\n",
    "    if 'SR' in row[0]:\n",
    "        labels.append(20)\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(115):\n",
    "    random_index = random.randrange(len(labels))\n",
    "    x_test.append(texts.pop(random_index))\n",
    "    y_test.append(labels.pop(random_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f0b630f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "34/34 [==============================] - 7s 141ms/step - loss: 2.8874 - accuracy: 0.1220\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 5s 134ms/step - loss: 2.7039 - accuracy: 0.1555\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 5s 132ms/step - loss: 2.5378 - accuracy: 0.2179\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 4s 131ms/step - loss: 2.2460 - accuracy: 0.2905\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 5s 134ms/step - loss: 1.9510 - accuracy: 0.4488\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 4s 132ms/step - loss: 1.7132 - accuracy: 0.5298\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 4s 131ms/step - loss: 1.4612 - accuracy: 0.5894\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 5s 134ms/step - loss: 1.2675 - accuracy: 0.6750\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 4s 131ms/step - loss: 1.1325 - accuracy: 0.7207\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - 5s 133ms/step - loss: 0.9836 - accuracy: 0.7439\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - 4s 130ms/step - loss: 0.8820 - accuracy: 0.7831\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - 4s 129ms/step - loss: 0.6973 - accuracy: 0.8417\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - 5s 133ms/step - loss: 0.6502 - accuracy: 0.8538\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - 4s 127ms/step - loss: 0.5637 - accuracy: 0.8808\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - 5s 137ms/step - loss: 0.4326 - accuracy: 0.9022\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - 4s 130ms/step - loss: 0.3809 - accuracy: 0.9041\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - 4s 128ms/step - loss: 0.3461 - accuracy: 0.9209\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - 4s 129ms/step - loss: 0.3331 - accuracy: 0.9209\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - 4s 128ms/step - loss: 0.3204 - accuracy: 0.9348\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - 4s 129ms/step - loss: 0.3444 - accuracy: 0.9218\n",
      "4/4 [==============================] - 1s 59ms/step - loss: 4.0712 - accuracy: 0.1304\n",
      "Test set\n",
      "  Loss: 4.071\n",
      "  Accuracy: 0.130\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "max_words = 15000 # max number of words to use in the vocabulary\n",
    "max_len = 250 # max length of each text (in terms of number of words)\n",
    "embedding_dim = 100 # dimension of word embeddings\n",
    "lstm_units = 64 # number of units in the LSTM layer\n",
    "num_classes = len(set(labels)) + 1 # number of classes\n",
    "#num_classes2 = len(set(y_test)) + 1\n",
    "\n",
    "# Tokenize the texts and create a vocabulary\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad the sequences so they all have the same length\n",
    "x = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "\n",
    "# Create one-hot encoded labels\n",
    "y = keras.utils.to_categorical(labels, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=max_len))\n",
    "model.add(LSTM(lstm_units))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x, y, batch_size=32, epochs=20)\n",
    "\n",
    "x_test = list(x_test)\n",
    "# Tokenize the texts and create a vocabulary\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(x_test)\n",
    "sequences = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "\n",
    "# Pad the sequences so they all have the same length\n",
    "x_test = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "#Test the model\n",
    "accr = model.evaluate(x_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "\n",
    "#for i in range(10):\n",
    "#    result = tf.argmax(model.predict(tf.expand_dims(texts[i], 0)), axis=1)\n",
    "#    print(result.numpy(), labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "28cea26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(115, 250)\n",
      "(115, 21)\n",
      "(1074, 250)\n",
      "(1074, 21)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(y_test)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0effd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
